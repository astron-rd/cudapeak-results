
# CUDA Peak Performance Report

This auto-generated report presents benchmark results for the [cudapeak](https://github.com/astron-rd/cudapeak) FP32 and MMA synthetic performance benchmarks on various GPUs.

The benchmarks evaluate synthetic workloads designed to measure peak operations per second (OPs), providing insights into architectural efficiency and computational limits.

## Absolute Performance
Measured in teraoperations per second (TOPs), showing raw computational throughput for various data types and MMA sizes.

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>gpu</th>
      <th>Blackwell Thor</th>
      <th>Ampere A4000</th>
      <th>Ampere A100</th>
      <th>Ada 4000</th>
      <th>Ada RTX4070</th>
      <th>Blackwell RTX5070</th>
      <th>Ada 5000</th>
      <th>Blackwell 6000</th>
    </tr>
    <tr>
      <th>name</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fp32</th>
      <td>7.01929</td>
      <td>18.930950</td>
      <td>19.067255</td>
      <td>26.407110</td>
      <td>28.797527</td>
      <td>30.819866</td>
      <td>64.498883</td>
      <td>109.655750</td>
    </tr>
    <tr>
      <th>mma_tf32_16_16_8</th>
      <td>14.18230</td>
      <td>37.903967</td>
      <td>149.914518</td>
      <td>52.844960</td>
      <td>29.114508</td>
      <td>15.454058</td>
      <td>129.080338</td>
      <td>110.215085</td>
    </tr>
    <tr>
      <th>mma_bf16_16_16_16</th>
      <td>56.41290</td>
      <td>75.212588</td>
      <td>288.862000</td>
      <td>104.852255</td>
      <td>58.160180</td>
      <td>61.804632</td>
      <td>256.103896</td>
      <td>440.483894</td>
    </tr>
    <tr>
      <th>mma_f16_16_16_16</th>
      <td>56.63970</td>
      <td>76.615314</td>
      <td>308.900588</td>
      <td>106.844700</td>
      <td>58.274648</td>
      <td>61.802681</td>
      <td>260.972078</td>
      <td>440.530013</td>
    </tr>
    <tr>
      <th>mma_e4m3_16_8_32</th>
      <td>171.29000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>213.455950</td>
      <td>116.452746</td>
      <td>123.545516</td>
      <td>521.370130</td>
      <td>878.445664</td>
    </tr>
    <tr>
      <th>mma_e5m2_16_8_32</th>
      <td>171.28500</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>213.455950</td>
      <td>116.452617</td>
      <td>123.544432</td>
      <td>521.370130</td>
      <td>878.438458</td>
    </tr>
    <tr>
      <th>mma_s8_16_8_32</th>
      <td>112.79600</td>
      <td>153.317012</td>
      <td>618.888627</td>
      <td>213.654600</td>
      <td>233.112773</td>
      <td>246.765783</td>
      <td>521.855844</td>
      <td>880.022351</td>
    </tr>
    <tr>
      <th>mma_s8_16_16_16</th>
      <td>56.75000</td>
      <td>152.894220</td>
      <td>618.199294</td>
      <td>213.170300</td>
      <td>232.464375</td>
      <td>123.608397</td>
      <td>520.672468</td>
      <td>440.795196</td>
    </tr>
    <tr>
      <th>mma_e2m1_16_8_64</th>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>490.279085</td>
      <td>0.000000</td>
      <td>1755.167636</td>
    </tr>
    <tr>
      <th>mma_e3m2_16_8_32</th>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>123.546600</td>
      <td>0.000000</td>
      <td>878.454311</td>
    </tr>
    <tr>
      <th>mma_s4_8_8_32</th>
      <td>7.06834</td>
      <td>303.516571</td>
      <td>617.205843</td>
      <td>423.168000</td>
      <td>461.474063</td>
      <td>31.767964</td>
      <td>1033.597792</td>
      <td>112.956561</td>
    </tr>
    <tr>
      <th>bmma_b1_8_8_128_and</th>
      <td>31.16420</td>
      <td>1186.334367</td>
      <td>2412.574510</td>
      <td>1654.073000</td>
      <td>1803.798047</td>
      <td>107.446707</td>
      <td>4040.061039</td>
      <td>383.101972</td>
    </tr>
    <tr>
      <th>bmma_b1_8_8_128_xor</th>
      <td>11.89660</td>
      <td>1214.064163</td>
      <td>2468.845490</td>
      <td>1692.686500</td>
      <td>1845.898828</td>
      <td>47.764211</td>
      <td>4134.355844</td>
      <td>170.378837</td>
    </tr>
    <tr>
      <th>bmma_b1_16_8_256_and</th>
      <td>71.19540</td>
      <td>1226.542041</td>
      <td>4951.127451</td>
      <td>1709.245500</td>
      <td>1864.886719</td>
      <td>232.242369</td>
      <td>4174.846753</td>
      <td>808.865310</td>
    </tr>
    <tr>
      <th>bmma_b1_16_8_256_xor</th>
      <td>20.78760</td>
      <td>1226.542041</td>
      <td>4951.109020</td>
      <td>1709.245500</td>
      <td>1864.886719</td>
      <td>82.348065</td>
      <td>4174.868831</td>
      <td>293.480101</td>
    </tr>
  </tbody>
</table>

## Normalized Performance  
Scaled relative to each GPU's slowest individual benchmark result, revealing how performance scales across different data types.

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>gpu</th>
      <th>Ada 4000</th>
      <th>Ada 5000</th>
      <th>Ada RTX4070</th>
      <th>Ampere A100</th>
      <th>Ampere A4000</th>
      <th>Blackwell 6000</th>
      <th>Blackwell Thor</th>
      <th>Blackwell RTX5070</th>
    </tr>
    <tr>
      <th>name</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fp32</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.994289</td>
    </tr>
    <tr>
      <th>mma_tf32_16_16_8</th>
      <td>2.001164</td>
      <td>2.001280</td>
      <td>1.011007</td>
      <td>7.862407</td>
      <td>2.002222</td>
      <td>1.005101</td>
      <td>2.020475</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>mma_bf16_16_16_16</th>
      <td>3.970607</td>
      <td>3.970672</td>
      <td>2.019624</td>
      <td>15.149638</td>
      <td>3.972996</td>
      <td>4.016970</td>
      <td>8.036838</td>
      <td>3.999249</td>
    </tr>
    <tr>
      <th>mma_f16_16_16_16</th>
      <td>4.046058</td>
      <td>4.046149</td>
      <td>2.023599</td>
      <td>16.200580</td>
      <td>4.047093</td>
      <td>4.017391</td>
      <td>8.069149</td>
      <td>3.999123</td>
    </tr>
    <tr>
      <th>mma_e4m3_16_8_32</th>
      <td>8.083276</td>
      <td>8.083398</td>
      <td>4.043845</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>8.010940</td>
      <td>24.402753</td>
      <td>7.994374</td>
    </tr>
    <tr>
      <th>mma_e5m2_16_8_32</th>
      <td>8.083276</td>
      <td>8.083398</td>
      <td>4.043841</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>8.010875</td>
      <td>24.402041</td>
      <td>7.994304</td>
    </tr>
    <tr>
      <th>mma_s8_16_8_32</th>
      <td>8.090798</td>
      <td>8.090928</td>
      <td>8.094889</td>
      <td>32.458192</td>
      <td>8.098749</td>
      <td>8.025319</td>
      <td>16.069432</td>
      <td>15.967701</td>
    </tr>
    <tr>
      <th>mma_s8_16_16_16</th>
      <td>8.072459</td>
      <td>8.072581</td>
      <td>8.072373</td>
      <td>32.422040</td>
      <td>8.076416</td>
      <td>4.019809</td>
      <td>8.084863</td>
      <td>7.998443</td>
    </tr>
    <tr>
      <th>mma_e2m1_16_8_64</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>16.006161</td>
      <td>0.000000</td>
      <td>31.724941</td>
    </tr>
    <tr>
      <th>mma_e3m2_16_8_32</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>8.011019</td>
      <td>0.000000</td>
      <td>7.994444</td>
    </tr>
    <tr>
      <th>mma_s4_8_8_32</th>
      <td>16.024775</td>
      <td>16.025049</td>
      <td>16.024781</td>
      <td>32.369937</td>
      <td>16.032823</td>
      <td>1.030102</td>
      <td>1.006988</td>
      <td>2.055639</td>
    </tr>
    <tr>
      <th>bmma_b1_8_8_128_and</th>
      <td>62.637411</td>
      <td>62.637690</td>
      <td>62.637254</td>
      <td>126.529725</td>
      <td>62.666393</td>
      <td>3.493679</td>
      <td>4.439794</td>
      <td>6.952653</td>
    </tr>
    <tr>
      <th>bmma_b1_8_8_128_xor</th>
      <td>64.099650</td>
      <td>64.099650</td>
      <td>64.099213</td>
      <td>129.480909</td>
      <td>64.131179</td>
      <td>1.553761</td>
      <td>1.694844</td>
      <td>3.090723</td>
    </tr>
    <tr>
      <th>bmma_b1_16_8_256_and</th>
      <td>64.726716</td>
      <td>64.727427</td>
      <td>64.758571</td>
      <td>259.666506</td>
      <td>64.790305</td>
      <td>7.376406</td>
      <td>10.142821</td>
      <td>15.027921</td>
    </tr>
    <tr>
      <th>bmma_b1_16_8_256_xor</th>
      <td>64.726716</td>
      <td>64.727769</td>
      <td>64.758571</td>
      <td>259.665539</td>
      <td>64.790305</td>
      <td>2.676377</td>
      <td>2.961496</td>
      <td>5.328572</td>
    </tr>
  </tbody>
</table>

*Report generated on 2025-11-06 15:21 UTC*
